{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data to pandas dataframes\n",
    "train_df = pd.read_csv(\"data/airline_train.csv\")\n",
    "test_df = pd.read_csv(\"data/airline_test.csv\")\n",
    "\n",
    "# convert the string representation of a list to a list\n",
    "train_df[\"airlines\"] = train_df[\"airlines\"].apply(lambda x: literal_eval(x))\n",
    "test_df[\"airlines\"] = test_df[\"airlines\"].apply(lambda x: literal_eval(x))\n",
    "\n",
    "# Combine the train and test dataframes, we are not splitting the data into train and test sets because we are not training a traditional ML model \n",
    "# Adding the train set gives us a better evaluation set\n",
    "train_test_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "  api_key=os.getenv('OPENAI_API_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_ner_response(client: OpenAI, tweet_text: str, model=\"gpt-3.5-turbo\") -> str:\n",
    "\n",
    "    # system prompt\n",
    "    system_prompt = \"\"\"You are an expert NER model. Your task is to read a tweet and identify any airline\n",
    "                    companies mentioned. Return a JSON object with a list of identified airline names.\n",
    "                    If no airline names are found, return an empty list. Do not include other types of entities.\n",
    "                    Consider common abbreviations (e.g., 'AA' for American Airlines) as well, if relevant.\"\"\"\n",
    "\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    Below are examples of tweets and the corresponding JSON output.\n",
    "\n",
    "    Example 1:\n",
    "    Tweet: \"Had a terrible experience with United yesterday!\"\n",
    "    Output: {{\"airlines\": [\"United Airlines\"]}}\n",
    "\n",
    "    Example 2:\n",
    "    Tweet: \"I just booked a flight on Southwest, I hope it goes well.\"\n",
    "    Output: {{\"airlines\": [\"Southwest Airlines\"]}}\n",
    "\n",
    "    Example 3:\n",
    "    Tweet: \"AA lost my luggage again.\"\n",
    "    Output: {{\"airlines\": [\"American Airlines\"]}}\n",
    "\n",
    "    Example 4:\n",
    "    Tweet: \"No airline can beat Emirates in terms of luxury.\"\n",
    "    Output: {{\"airlines\": [\"Emirates\"]}}\n",
    "\n",
    "    ---\n",
    "    Now, follow the exact same format and process the tweet below.\n",
    "    Tweet: \"{tweet_text}\"\n",
    "\n",
    "    Remember:\n",
    "    1. Only list airline names if they are explicitly or implicitly mentioned.\n",
    "    2. Consider common abbreviations or short forms.\n",
    "    3. Return the output as a JSON object with the key 'airlines' pointing to a list of strings.\n",
    "    4. If no airline name is mentioned, return {{\"airlines\": []}}.\n",
    "    Remember to include the full airline name, not just the abbreviation.\n",
    "    Some airlines may be misspelled in the tweet, but you should still include them in the output with the correct name.\"\"\"\n",
    "\n",
    "\n",
    "    # Call the OpenAI API\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,  # For more deterministic output\n",
    "    )\n",
    "\n",
    "    # print(completion.choices[0].message) #debug\n",
    "\n",
    "    return completion.choices[0].message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Southwest Airlines My bags are on the way to Chicago, without me! Help! I was confirmed for 2 flights and told there isn't room and I'm screwed.\n",
      "{\n",
      "    \"airlines\": [\"Southwest Airlines\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "\n",
    "tweet = train_test_df[\"tweet\"][3]\n",
    "print(tweet)\n",
    "response = get_openai_ner_response(client, tweet)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch process the tweets for a whole dataframe\n",
    "def get_openai_ner_response_batch(client: OpenAI, df: pd.DataFrame, model=\"gpt-3.5-turbo\") -> list:\n",
    "    responses = []\n",
    "    for i, tweet_text in enumerate(df[\"tweet\"]):\n",
    "        print(f\"Processing tweet {i+1}/{len(df)}\")\n",
    "        try:\n",
    "            response = get_openai_ner_response(client, tweet_text, model)\n",
    "            response_json = json.loads(response.content)\n",
    "            responses.append(list(set(response_json[\"airlines\"])))\n",
    "        except Exception as e:\n",
    "            responses.append(\"\")\n",
    "            print(f\"Error processing tweet: {e}\")\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run NER extraction over the dataframe\n",
    "# here, we are running only the test set because we are rate limited\n",
    "test_df[\"airlines_predicted\"] = get_openai_ner_response_batch(client, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompt_responses(df):\n",
    "\n",
    "    # Convert the columns to sets so we can compare the extracted entities.\n",
    "    labels = df[\"airlines\"].apply(lambda x: set(x))\n",
    "    predictions = df[\"airlines_predicted\"].apply(lambda x: set(x))\n",
    "\n",
    "    # compare the two lists of sets and calculate the percentage accuracy, the false positive rate, and the false negative rate\n",
    "    # Accuracy is defined as the percentage of sets that are equal\n",
    "    # False positive rate is the percentage of sets in the predictions that are not in the labels\n",
    "    # False negative rate is the percentage of sets in the labels that are not in the predictions\n",
    "\n",
    "    acc, fp, fn = [],[],[]\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        # Count accurate predictions\n",
    "        if labels[i] == predictions[i]:\n",
    "            acc.append(1)\n",
    "        else:\n",
    "            acc.append(0)\n",
    "\n",
    "        # Count predictions that are not in the labels\n",
    "        if predictions[i] - labels[i]:\n",
    "            fp.append(1)\n",
    "        else:\n",
    "            fp.append(0)\n",
    "\n",
    "        # Count labels that are not in the predictions\n",
    "        if labels[i] - predictions[i]:\n",
    "            fn.append(1)\n",
    "        else:\n",
    "            fn.append(0)\n",
    "\n",
    "    accuracy = \"{:.2f}\".format(sum(acc) / len(df))\n",
    "    false_positive_rate = \"{:.2f}\".format(sum(fp) / len(df))\n",
    "    false_negative_rate = \"{:.2f}\".format(sum(fn) / len(df))\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"False Positive Rate: {false_positive_rate}\")\n",
    "    print(f\"False Negative Rate: {false_negative_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91\n",
      "False Positive Rate: 0.05\n",
      "False Negative Rate: 0.05\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.read_csv(\"results/results_gpt3_5.csv\")\n",
    "evaluate_prompt_responses(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Evaluation Metrics:*\n",
    "\n",
    "- **Accuracy:** 0.91 -- This means 91 out of every 100 tweets had extracted all airlines perfectly\n",
    "\n",
    "- **False Positive Rate:** 0.05 -- This means 5 out of every 100 tweets had picked an airline name that wasn't found in the labels\n",
    "\n",
    "- **False Negative Rate:** 0.05 -- This means 5 out of every 100 tweets had failed to find the airline name(s) in the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions for the test_df using the GPT-4o model\n",
    "gpt4_test_df = test_df.copy()\n",
    "gpt4_test_df[\"airlines_predicted\"] = get_openai_ner_response_batch(client, test_df, \"gpt-4o-mini-2024-07-18\")\n",
    "\n",
    "# Save the results to a CSV file\n",
    "gpt4_test_df.to_csv(\"results/results_gpt4o.csv\", index=False)\n",
    "\n",
    "### If you get rate limited with a 429 error, you can try again after waiting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
